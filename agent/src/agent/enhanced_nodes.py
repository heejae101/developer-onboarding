"""
Enhanced Agent Nodes - Self-RAG, ë³‘ë ¬ê²€ìƒ‰, Answer Grading íŒ¨í„´ ë…¸ë“œ
"""
from src.agent.state import AgentState
from src.llm import get_llm_client
from src.graph_settings import get_graph_settings
import json
import asyncio


# ============================================================
# Self-RAG íŒ¨í„´ ë…¸ë“œ
# ============================================================

async def evaluate_node(state: AgentState) -> AgentState:
    """
    RAG ê²°ê³¼ì˜ ê´€ë ¨ì„±ê³¼ ì¶©ë¶„ì„±ì„ í‰ê°€í•˜ëŠ” ë…¸ë“œ
    - ê´€ë ¨ì„±ì´ ë‚®ìœ¼ë©´ ì¬ê²€ìƒ‰ íŠ¸ë¦¬ê±°
    """
    settings = get_graph_settings()
    llm = get_llm_client()
    
    rag_results = state.get("rag_results", [])
    message = state.get("message", "")
    
    if not rag_results:
        # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì¬ê²€ìƒ‰ í•„ìš”
        state["is_relevant"] = False
        state["relevance_score"] = 0.0
        return state
    
    # LLMìœ¼ë¡œ ê´€ë ¨ì„± í‰ê°€
    context = "\n".join([r.get("content", "") for r in rag_results[:3]])
    
    prompt = f"""ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì‚¬ìš©ì ì§ˆë¬¸ì— ì¶©ë¶„íˆ ê´€ë ¨ì´ ìˆëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.

[ì‚¬ìš©ì ì§ˆë¬¸]
{message}

[ê²€ìƒ‰ ê²°ê³¼]
{context[:2000]}

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
    "relevance_score": 0.0~1.0 (ê´€ë ¨ì„± ì ìˆ˜),
    "is_sufficient": true/false (ë‹µë³€í•˜ê¸°ì— ì¶©ë¶„í•œì§€),
    "reason": "í‰ê°€ ì´ìœ ",
    "suggested_query": "ë” ì¢‹ì€ ê²€ìƒ‰ ì¿¼ë¦¬ (í•„ìš”ì‹œ)"
}}"""

    response = llm.chat([
        {"role": "system", "content": "You are a relevance evaluator. Respond only with JSON."},
        {"role": "user", "content": prompt}
    ])
    
    try:
        content = _extract_content(response)
        result = json.loads(content)
        
        state["relevance_score"] = result.get("relevance_score", 0.5)
        state["is_relevant"] = (
            result.get("is_sufficient", True) and 
            state["relevance_score"] >= settings.relevance_threshold
        )
        
        # ì¬ê²€ìƒ‰ì´ í•„ìš”í•˜ë©´ ì¿¼ë¦¬ ìˆ˜ì •
        if not state["is_relevant"] and result.get("suggested_query"):
            state["search_query"] = result["suggested_query"]
            
    except (json.JSONDecodeError, KeyError):
        state["is_relevant"] = True  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì§„í–‰
        state["relevance_score"] = 0.5
    
    if settings.enable_step_logging:
        print(f"ğŸ“Š [Evaluate] relevance={state['relevance_score']:.2f}, relevant={state['is_relevant']}")
    
    return state


def should_retry_search(state: AgentState) -> str:
    """Self-RAG: ì¬ê²€ìƒ‰ ì—¬ë¶€ ê²°ì •"""
    settings = get_graph_settings()
    
    attempts = state.get("search_attempts", 0)
    is_relevant = state.get("is_relevant", True)
    
    if not is_relevant and attempts < settings.max_search_retries:
        return "retry"
    return "continue"


# ============================================================
# ë³‘ë ¬ ê²€ìƒ‰ íŒ¨í„´ ë…¸ë“œ
# ============================================================

async def parallel_search_node(state: AgentState) -> AgentState:
    """
    RAG + íŒŒì¼ê²€ìƒ‰ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ëŠ” ë…¸ë“œ
    """
    from src.agent.tools import RuleSearchTool, FileSearchTool
    
    settings = get_graph_settings()
    message = state.get("message", "")
    search_query = state.get("search_query", message)
    
    # ê²€ìƒ‰ ì‹œë„ íšŸìˆ˜ ì¦ê°€
    state["search_attempts"] = state.get("search_attempts", 0) + 1
    
    def search_rag():
        tool = RuleSearchTool()
        result = tool.search(search_query)
        return {"source": "rag", "content": result}
    
    def search_files():
        tool = FileSearchTool()
        results = tool.search_files(search_query)
        return {"source": "file", "results": results}
    
    # ë³‘ë ¬ ì‹¤í–‰
    if settings.enable_parallel_search:
        rag_task = asyncio.create_task(asyncio.to_thread(search_rag))
        file_task = asyncio.create_task(asyncio.to_thread(search_files))
        
        rag_result, file_result = await asyncio.gather(rag_task, file_task)
    else:
        rag_result = await asyncio.to_thread(search_rag)
        file_result = {"source": "file", "results": []}
    
    state["rag_results"] = [rag_result] if rag_result else []
    state["file_results"] = file_result.get("results", [])
    
    if settings.enable_step_logging:
        print(f"ğŸ” [Search] RAG={len(state['rag_results'])}, Files={len(state['file_results'])}")
    
    return state


async def synthesize_node(state: AgentState) -> AgentState:
    """
    ì—¬ëŸ¬ ì†ŒìŠ¤ì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í†µí•©í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    """
    settings = get_graph_settings()
    llm = get_llm_client()
    
    rag_results = state.get("rag_results", [])
    file_results = state.get("file_results", [])
    message = state.get("message", "")
    
    # RAG ê²°ê³¼ ì¶”ì¶œ
    rag_context = ""
    for r in rag_results:
        if isinstance(r, dict):
            rag_context += r.get("content", "") + "\n"
    
    # íŒŒì¼ ê²°ê³¼ í¬ë§·
    file_context = ""
    if file_results:
        file_context = "ê´€ë ¨ íŒŒì¼:\n" + "\n".join([
            f"- {f.get('name', '')} ({f.get('path', '')})" 
            for f in file_results[:5]
        ])
    
    # LLMìœ¼ë¡œ í†µí•© ì‘ë‹µ ìƒì„±
    prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

[ì§ˆë¬¸]
{message}

[í”„ë¡œì íŠ¸ ê·œì¹™/ë¬¸ì„œ]
{rag_context[:3000]}

[ê´€ë ¨ íŒŒì¼]
{file_context}

ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."""

    response = llm.chat([
        {"role": "system", "content": "You are a helpful developer assistant."},
        {"role": "user", "content": prompt}
    ])
    
    content = _extract_content(response)
    state["final_response"] = content
    state["combined_context"] = rag_context + "\n" + file_context
    
    if settings.enable_step_logging:
        print(f"ğŸ“ [Synthesize] Generated response ({len(content)} chars)")
    
    return state


# ============================================================
# Answer Grading íŒ¨í„´ ë…¸ë“œ
# ============================================================

async def grade_answer_node(state: AgentState) -> AgentState:
    """
    ìƒì„±ëœ ë‹µë³€ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ë…¸ë“œ
    """
    settings = get_graph_settings()
    llm = get_llm_client()
    
    answer = state.get("final_response", "")
    message = state.get("message", "")
    context = state.get("combined_context", "")
    
    prompt = f"""ìƒì„±ëœ ë‹µë³€ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ì„¸ìš”.

[ì‚¬ìš©ì ì§ˆë¬¸]
{message}

[ì°¸ì¡° ì»¨í…ìŠ¤íŠ¸]
{context[:1500]}

[ìƒì„±ëœ ë‹µë³€]
{answer}

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ê³  JSONìœ¼ë¡œ ì‘ë‹µ:
1. ì •í™•ì„±: ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œê°€?
2. ì™„ì „ì„±: ì§ˆë¬¸ì— ì¶©ë¶„íˆ ë‹µë³€í–ˆëŠ”ê°€?
3. ëª…í™•ì„±: ì´í•´í•˜ê¸° ì‰¬ìš´ê°€?

{{
    "score": 0.0~1.0,
    "is_acceptable": true/false,
    "feedback": "ê°œì„ ì´ í•„ìš”í•œ ì ",
    "missing_info": ["ë¹ ì§„ ì •ë³´ ëª©ë¡"]
}}"""

    response = llm.chat([
        {"role": "system", "content": "You are a quality evaluator. Respond only with JSON."},
        {"role": "user", "content": prompt}
    ])
    
    try:
        content = _extract_content(response)
        result = json.loads(content)
        
        state["answer_score"] = result.get("score", 0.7)
        state["grading_feedback"] = result.get("feedback", "")
        
    except (json.JSONDecodeError, KeyError):
        state["answer_score"] = 0.75  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í†µê³¼
        state["grading_feedback"] = ""
    
    if settings.enable_step_logging:
        print(f"â­ [Grade] score={state['answer_score']:.2f}")
    
    return state


def should_refine_answer(state: AgentState) -> str:
    """Answer Grading: ë‹µë³€ ê°œì„  ì—¬ë¶€ ê²°ì •"""
    settings = get_graph_settings()
    
    score = state.get("answer_score", 1.0)
    attempts = state.get("refine_attempts", 0)
    
    if score < settings.min_answer_score and attempts < settings.max_refine_attempts:
        return "refine"
    return "complete"


async def refine_answer_node(state: AgentState) -> AgentState:
    """
    í’ˆì§ˆì´ ë‚®ì€ ë‹µë³€ì„ ê°œì„ í•˜ëŠ” ë…¸ë“œ
    """
    settings = get_graph_settings()
    llm = get_llm_client()
    
    state["refine_attempts"] = state.get("refine_attempts", 0) + 1
    
    answer = state.get("final_response", "")
    feedback = state.get("grading_feedback", "")
    message = state.get("message", "")
    context = state.get("combined_context", "")
    
    prompt = f"""ë‹µë³€ì„ ê°œì„ í•˜ì„¸ìš”.

[ì‚¬ìš©ì ì§ˆë¬¸]
{message}

[í˜„ì¬ ë‹µë³€]
{answer}

[ê°œì„  í”¼ë“œë°±]
{feedback}

[ì°¸ì¡° ì»¨í…ìŠ¤íŠ¸]
{context[:1500]}

í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ë” ë‚˜ì€ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”."""

    response = llm.chat([
        {"role": "system", "content": "You are a helpful assistant improving your previous answer."},
        {"role": "user", "content": prompt}
    ])
    
    content = _extract_content(response)
    state["final_response"] = content
    
    if settings.enable_step_logging:
        print(f"ğŸ”„ [Refine] Attempt {state['refine_attempts']}")
    
    return state


# ============================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================

def _extract_content(response) -> str:
    """ë‹¤ì–‘í•œ LLM ì‘ë‹µ í˜•ì‹ì—ì„œ ì»¨í…ì¸  ì¶”ì¶œ"""
    try:
        if hasattr(response, 'choices'):  # OpenAI
            return response.choices[0].message.content
        elif hasattr(response, 'content'):  # Anthropic
            if isinstance(response.content, list):
                return response.content[0].text
            return response.content
        elif isinstance(response, dict):  # Ollama
            return response.get("message", {}).get("content", str(response))
        else:
            return str(response)
    except (AttributeError, IndexError, KeyError):
        return str(response)
