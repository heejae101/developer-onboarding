# [ìƒì„¸ ì„¤ê³„] LangGraph ë…¸ë“œ ìƒì„¸ ì„¤ê³„ v2.0

## 1. ê°œìš”

LangGraph ê¸°ë°˜ì˜ ì§€ëŠ¥í˜• ì—ì´ì „íŠ¸ êµ¬ì¡°ì…ë‹ˆë‹¤. **v2.0**ì—ì„œëŠ” ììœ¨ ì—ì´ì „íŠ¸ ë£¨í”„(Autonomous Loop), LLM ëª¨ë“œ ìŠ¤ìœ„ì¹­(API/Ollama), **ì½”ë“œ ê²€ìˆ˜ ì—ì´ì „íŠ¸**ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

> [!TIP] LLM ëª¨ë“œ ìŠ¤ìœ„ì¹­
> - ì´ˆê¸°: OpenAI/Anthropic API ì‚¬ìš©
> - ì•ˆì •í™” í›„: Ollama ë¡œì»¬ ëª¨ë“œ ì „í™˜

---

## 2. ì „ì²´ ê·¸ë˜í”„ êµ¬ì¡°

```mermaid
graph TD
    subgraph "Entry Layer"
        Start((ì‹œì‘)) --> Input[User Input]
        Input --> Router{Router Node}
    end
    
    subgraph "Single Query Path"
        Router -->|ì§ˆë¬¸/ìƒë‹´| RAG[Search Rules Node]
        Router -->|ì½”ë“œ/ë¬¸ì„œ ê²€ì¦| Verify[Verify Rules Node]
        Router -->|ì½”ë“œ ê²€ìˆ˜| CodeReview[Code Review Agent]
        RAG --> Summarize[Summarize Response]
        Verify --> Fix[Suggest Fix]
        CodeReview --> ReviewResult[Review Result]
        Summarize --> StreamOut[Stream Output]
        Fix --> StreamOut
        ReviewResult --> StreamOut
    end
    
    subgraph "Autonomous Agent Loop"
        Router -->|ë³µì¡í•œ ì‘ì—…| Think[Think Node]
        Think --> Plan[Plan Node]
        Plan --> Act[Act Node]
        Act --> Observe[Observe Node]
        Observe --> ShouldContinue{ê³„ì†?}
        ShouldContinue -->|Yes| Think
        ShouldContinue -->|No| Complete[Complete Node]
        Complete --> StreamOut
    end
    
    subgraph "Action Layer"
        Act --> Tools{MCP Tools}
        Tools --> FileOps[File Operations]
        Tools --> DBOps[DB Operations]
        Tools --> Notify[Notification]
    end
    
    StreamOut --> End((ì¢…ë£Œ))
```

---

## 3. ë…¸ë“œë³„ ìƒì„¸ ì •ì˜

### 3.1 Router Node (ì˜ë„ íŒŒì•…)

```python
def router_node(state: AgentState) -> AgentState:
    """ì‚¬ìš©ì ì…ë ¥ì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ì—¬ ë‹¤ìŒ ë…¸ë“œ ê²°ì •"""
    prompt = f"""
    ì‚¬ìš©ì ì…ë ¥: {state.message}
    
    ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”:
    - SEARCH: ë‹¨ìˆœ ì§ˆë¬¸/ê·œì¹™ ê²€ìƒ‰
    - VERIFY: ì½”ë“œ/ë¬¸ì„œ ê²€ì¦ ìš”ì²­
    - CODE_REVIEW: ì½”ë“œ ê²€ìˆ˜ ìš”ì²­ (ìŠ¤íƒ€ì¼, ë²„ê·¸, ì„±ëŠ¥)
    - AUTONOMOUS: ë³µì¡í•œ ì‘ì—… (íŒŒì¼ ìƒì„±, ë‹¤ë‹¨ê³„ ë¶„ì„ ë“±)
    """
    
    response = llm_client.chat(messages=[...])
    state.next_node = parse_intent(response)
    return state
```

| ì…ë ¥ | ë¼ìš°íŒ… ê²°ê³¼ |
|:---|:---|
| "API ê·œì¹™ ì•Œë ¤ì¤˜" | SEARCH â†’ RAG |
| "ì´ ì½”ë“œ ìŠ¤íƒ€ì¼ ë§ì•„?" | VERIFY â†’ Verify Rules |
| "ì´ ì½”ë“œ ë¦¬ë·°í•´ì¤˜" | CODE_REVIEW â†’ Code Review Agent |
| "Controller ë§Œë“¤ì–´ì¤˜" | AUTONOMOUS â†’ Think |

### 3.2 Search Rules Node (RAG)

```python
def search_rules_node(state: AgentState) -> AgentState:
    """PROJECT_RULE í…Œì´ë¸”ì—ì„œ ê´€ë ¨ ê·œì¹™ ê²€ìƒ‰"""
    # 1. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± (Ollama)
    embedding = ollama.embeddings(model="nomic-embed-text", prompt=state.message)
    
    # 2. ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
    rules = db.search_similar_rules(embedding, top_k=3)
    
    state.retrieved_rules = rules
    return state
```

### 3.3 Verify Rules Node (ê²€ì¦)

```python
def verify_rules_node(state: AgentState) -> AgentState:
    """ì…ë ¥ëœ ì½”ë“œ/ë¬¸ì„œê°€ ê·œì¹™ì— ë¶€í•©í•˜ëŠ”ì§€ ê²€ì¦"""
    prompt = f"""
    [ê·œì¹™]
    {state.retrieved_rules}
    
    [ê²€ì¦ ëŒ€ìƒ]
    {state.user_code}
    
    ìœ„ ê·œì¹™ì— ë§ëŠ”ì§€ ê²€ì¦í•˜ê³ , ìœ„ë°˜ ì‚¬í•­ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•˜ì„¸ìš”.
    """
    
    response = llm_client.chat(messages=[...])
    state.validation_result = parse_validation(response)
    return state
```

### 3.4 Code Review Agent (ì½”ë“œ ê²€ìˆ˜) ğŸ†•

```python
def code_review_node(state: AgentState) -> AgentState:
    """ì½”ë“œ ê²€ìˆ˜ ì—ì´ì „íŠ¸ - ìŠ¤íƒ€ì¼, ë²„ê·¸, ì„±ëŠ¥, ë³´ì•ˆ ê²€í† """
    
    # 1. í”„ë¡œì íŠ¸ ê·œì¹™ ë¡œë“œ
    rules = db.get_all_rules(category="CONVENTION")
    
    prompt = f"""
    ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ê°œë°œìì…ë‹ˆë‹¤. ì•„ë˜ ì½”ë“œë¥¼ ê²€ìˆ˜í•´ì£¼ì„¸ìš”.
    
    [í”„ë¡œì íŠ¸ ì½”ë”© ì»¨ë²¤ì…˜]
    {rules}
    
    [ê²€ìˆ˜ ëŒ€ìƒ ì½”ë“œ]
    ```
    {state.user_code}
    ```
    
    ë‹¤ìŒ í•­ëª©ì„ ê²€í† í•˜ê³  JSON í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ì„¸ìš”:
    
    1. **ìŠ¤íƒ€ì¼ (style)**: ì½”ë”© ì»¨ë²¤ì…˜ ì¤€ìˆ˜ ì—¬ë¶€
    2. **ë²„ê·¸ (bugs)**: ì ì¬ì  ë²„ê·¸ ë° ì—ëŸ¬ ê°€ëŠ¥ì„±
    3. **ì„±ëŠ¥ (performance)**: ì„±ëŠ¥ ê°œì„  í¬ì¸íŠ¸
    4. **ë³´ì•ˆ (security)**: ë³´ì•ˆ ì·¨ì•½ì 
    5. **ê°€ë…ì„± (readability)**: ì½”ë“œ ê°€ë…ì„± ë° êµ¬ì¡°
    6. **ì´í‰ (summary)**: ì „ì²´ í‰ê°€ ë° ì ìˆ˜ (1-10)
    
    ê° í•­ëª©ë³„ë¡œ ë°œê²¬ëœ ì´ìŠˆì™€ ê°œì„  ì œì•ˆì„ í¬í•¨í•´ì£¼ì„¸ìš”.
    """
    
    response = llm_client.chat(messages=[
        {"role": "system", "content": "You are a senior code reviewer."},
        {"role": "user", "content": prompt}
    ])
    
    state.code_review_result = parse_review_result(response)
    return state

def parse_review_result(response) -> dict:
    """ë¦¬ë·° ê²°ê³¼ íŒŒì‹±"""
    return {
        "style": [],      # ìŠ¤íƒ€ì¼ ì´ìŠˆ ë¦¬ìŠ¤íŠ¸
        "bugs": [],       # ë²„ê·¸ ë¦¬ìŠ¤íŠ¸
        "performance": [],# ì„±ëŠ¥ ì´ìŠˆ ë¦¬ìŠ¤íŠ¸
        "security": [],   # ë³´ì•ˆ ì´ìŠˆ ë¦¬ìŠ¤íŠ¸
        "readability": [],# ê°€ë…ì„± ì´ìŠˆ ë¦¬ìŠ¤íŠ¸
        "summary": "",    # ì´í‰
        "score": 0        # ì ìˆ˜ (1-10)
    }
```

**ì½”ë“œ ê²€ìˆ˜ ê²°ê³¼ ì˜ˆì‹œ:**
```json
{
  "style": [
    {"line": 15, "issue": "@RequestMapping ëŒ€ì‹  @GetMapping ì‚¬ìš© ê¶Œì¥", "severity": "warning"}
  ],
  "bugs": [
    {"line": 23, "issue": "NullPointerException ê°€ëŠ¥ì„±", "severity": "error"}
  ],
  "performance": [
    {"line": 30, "issue": "N+1 ì¿¼ë¦¬ ë¬¸ì œ ë°œìƒ ê°€ëŠ¥", "severity": "warning"}
  ],
  "security": [],
  "readability": [
    {"line": 10, "issue": "ë©”ì„œë“œëª…ì´ ë¶ˆëª…í™•í•¨", "severity": "info"}
  ],
  "summary": "ì „ì²´ì ìœ¼ë¡œ ì–‘í˜¸í•˜ë‚˜ NPE ë°©ì–´ ì½”ë“œ í•„ìš”",
  "score": 7
}
```

---

## 4. ììœ¨ ì—ì´ì „íŠ¸ ë£¨í”„ (Autonomous Loop)

### 4.1 Think Node

```python
def think_node(state: AgentState) -> AgentState:
    """í˜„ì¬ ìƒíƒœë¥¼ ë¶„ì„í•˜ê³  ë‹¤ìŒ í–‰ë™ ê³„íš"""
    prompt = f"""
    [íƒœìŠ¤í¬] {state.task_description}
    [ì™„ë£Œëœ ë‹¨ê³„] {state.completed_steps}
    [í˜„ì¬ ê´€ì°°] {state.last_observation}
    
    ë‹¤ìŒì— ë¬´ì—‡ì„ í•´ì•¼ í•˜ëŠ”ì§€ ìƒê°í•˜ì„¸ìš”.
    """
    
    response = ollama.chat(model="llama3", messages=[...])
    state.current_thought = response
    return state
```

### 4.2 Plan Node

```python
def plan_node(state: AgentState) -> AgentState:
    """êµ¬ì²´ì ì¸ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½"""
    prompt = f"""
    [ìƒê°] {state.current_thought}
    
    ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬:
    - create_file(path, content): íŒŒì¼ ìƒì„±
    - read_file(path): íŒŒì¼ ì½ê¸°
    - search_rules(query): ê·œì¹™ ê²€ìƒ‰
    - execute_code(code): ì½”ë“œ ì‹¤í–‰
    
    ë‹¤ìŒ í–‰ë™ì„ JSON í˜•ì‹ìœ¼ë¡œ ê³„íší•˜ì„¸ìš”.
    """
    
    response = ollama.chat(model="llama3", messages=[...])
    state.action_plan = parse_action(response)
    return state
```

### 4.3 Act Node

```python
def act_node(state: AgentState) -> AgentState:
    """ê³„íšëœ í–‰ë™ ì‹¤í–‰ (MCP Tools í˜¸ì¶œ)"""
    action = state.action_plan
    
    if action.tool == "create_file":
        result = mcp_tools.create_file(action.path, action.content)
    elif action.tool == "read_file":
        result = mcp_tools.read_file(action.path)
    elif action.tool == "search_rules":
        result = search_rules_node(state)
    
    state.action_result = result
    return state
```

### 4.4 Observe Node

```python
def observe_node(state: AgentState) -> AgentState:
    """í–‰ë™ ê²°ê³¼ ê´€ì°° ë° ìƒíƒœ ì—…ë°ì´íŠ¸"""
    state.last_observation = state.action_result
    state.steps_completed += 1
    state.completed_steps.append({
        "thought": state.current_thought,
        "action": state.action_plan,
        "result": state.action_result
    })
    return state
```

### 4.5 Should Continue (ì¡°ê±´ë¶€ ì—£ì§€)

```python
def should_continue(state: AgentState) -> str:
    """ë£¨í”„ ê³„ì† ì—¬ë¶€ íŒë‹¨"""
    # ìµœëŒ€ ë‹¨ê³„ ë„ë‹¬
    if state.steps_completed >= state.max_steps:
        return "complete"
    
    # íƒœìŠ¤í¬ ì™„ë£Œ íŒë‹¨
    if "TASK_COMPLETE" in state.last_observation:
        return "complete"
    
    # ì—ëŸ¬ ë°œìƒ
    if state.error:
        return "complete"
    
    return "continue"
```

---

## 5. ê·¸ë˜í”„ ìƒíƒœ (State) ì •ì˜

```python
from typing import TypedDict, List, Optional

class AgentState(TypedDict):
    # ê¸°ë³¸ ì •ë³´
    thread_id: str
    user_id: int
    message: str
    
    # ë¼ìš°íŒ…
    next_node: str
    
    # RAG ê´€ë ¨
    retrieved_rules: List[dict]
    
    # ê²€ì¦ ê´€ë ¨
    user_code: Optional[str]
    validation_result: Optional[dict]
    
    # ììœ¨ ì—ì´ì „íŠ¸ ê´€ë ¨
    task_description: str
    max_steps: int
    steps_completed: int
    completed_steps: List[dict]
    current_thought: str
    action_plan: dict
    action_result: str
    last_observation: str
    error: Optional[str]
    
    # ì¶œë ¥
    final_response: str
    stream_tokens: List[str]
```

---

## 6. Ollama ë„¤ì´í‹°ë¸Œ ì—°ë™

```python
import os
from abc import ABC, abstractmethod

class LLMClient(ABC):
    """LLM í´ë¼ì´ì–¸íŠ¸ ì¶”ìƒ í´ë˜ìŠ¤"""
    @abstractmethod
    def chat(self, messages: list, stream: bool = False): pass
    
    @abstractmethod
    def chat_stream(self, messages: list): pass
    
    @abstractmethod
    def embed(self, text: str) -> list: pass

class APIClient(LLMClient):
    """OpenAI/Anthropic API í´ë¼ì´ì–¸íŠ¸ (ì´ˆê¸° ê°œë°œìš©)"""
    def __init__(self, provider: str = "openai"):
        if provider == "openai":
            from openai import OpenAI
            self.client = OpenAI()
            self.model = "gpt-4o-mini"
        else:
            from anthropic import Anthropic
            self.client = Anthropic()
            self.model = "claude-3-haiku-20240307"
        self.provider = provider
    
    def chat(self, messages: list, stream: bool = False):
        if self.provider == "openai":
            return self.client.chat.completions.create(
                model=self.model, messages=messages, stream=stream
            )
        else:
            return self.client.messages.create(
                model=self.model, messages=messages, stream=stream
            )
    
    def chat_stream(self, messages: list):
        response = self.chat(messages, stream=True)
        for chunk in response:
            if self.provider == "openai":
                yield chunk.choices[0].delta.content or ""
            else:
                yield chunk.delta.text or ""
    
    def embed(self, text: str) -> list:
        response = self.client.embeddings.create(
            model="text-embedding-3-small", input=text
        )
        return response.data[0].embedding

class OllamaClient(LLMClient):
    """Ollama ë¡œì»¬ í´ë¼ì´ì–¸íŠ¸ (ì•ˆì •í™” í›„ ì „í™˜)"""
    def __init__(self, model: str = "llama3"):
        import ollama
        self.ollama = ollama
        self.model = model
    
    def chat(self, messages: list, stream: bool = False):
        return self.ollama.chat(
            model=self.model, messages=messages, stream=stream
        )
    
    def chat_stream(self, messages: list):
        for chunk in self.ollama.chat(
            model=self.model, messages=messages, stream=True
        ):
            yield chunk["message"]["content"]
    
    def embed(self, text: str) -> list:
        return self.ollama.embeddings(
            model="nomic-embed-text", prompt=text
        )["embedding"]

def get_llm_client() -> LLMClient:
    """í™˜ê²½ ë³€ìˆ˜ì— ë”°ë¼ ì ì ˆí•œ LLM í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜"""
    mode = os.getenv("LLM_MODE", "api")  # ê¸°ë³¸ê°’: API ëª¨ë“œ
    provider = os.getenv("LLM_PROVIDER", "openai")
    
    if mode == "local":
        return OllamaClient(model=os.getenv("OLLAMA_MODEL", "llama3"))
    else:
        return APIClient(provider=provider)

# ì‚¬ìš© ì˜ˆì‹œ
llm_client = get_llm_client()
```

---

## 7. Git Worktree ë³‘ë ¬ ì—ì´ì „íŠ¸

```python
import subprocess
from pathlib import Path

class ParallelAgentManager:
    def __init__(self, base_repo: Path):
        self.base_repo = base_repo
        self.workers = []
    
    def create_worker(self, worker_id: str) -> Path:
        """ìƒˆ Worktree ìƒì„±"""
        worker_path = self.base_repo.parent / f"agent-worker-{worker_id}"
        subprocess.run([
            "git", "worktree", "add", str(worker_path), "main"
        ], cwd=self.base_repo)
        return worker_path
    
    def run_parallel_tasks(self, tasks: list):
        """ì—¬ëŸ¬ íƒœìŠ¤í¬ë¥¼ ë³‘ë ¬ ì‹¤í–‰"""
        import asyncio
        
        async def run_task(task, worker_path):
            agent = AutonomousAgent(working_dir=worker_path)
            return await agent.run(task)
        
        loop = asyncio.get_event_loop()
        results = loop.run_until_complete(
            asyncio.gather(*[
                run_task(task, self.create_worker(f"task-{i}"))
                for i, task in enumerate(tasks)
            ])
        )
        return results
    
    def cleanup_workers(self):
        """Worktree ì •ë¦¬"""
        for worker in self.workers:
            subprocess.run(["git", "worktree", "remove", str(worker)])
```

---

## 8. ê·¸ë˜í”„ ì»´íŒŒì¼ ë° ì‹¤í–‰

```python
from langgraph.graph import StateGraph, END

def build_agent_graph() -> StateGraph:
    graph = StateGraph(AgentState)
    
    # ë…¸ë“œ ì¶”ê°€
    graph.add_node("router", router_node)
    graph.add_node("search_rules", search_rules_node)
    graph.add_node("verify_rules", verify_rules_node)
    graph.add_node("summarize", summarize_node)
    graph.add_node("suggest_fix", suggest_fix_node)
    graph.add_node("think", think_node)
    graph.add_node("plan", plan_node)
    graph.add_node("act", act_node)
    graph.add_node("observe", observe_node)
    graph.add_node("complete", complete_node)
    graph.add_node("stream_output", stream_output_node)
    
    # ì—£ì§€ ì—°ê²°
    graph.set_entry_point("router")
    
    graph.add_conditional_edges("router", route_by_intent, {
        "SEARCH": "search_rules",
        "VERIFY": "verify_rules",
        "AUTONOMOUS": "think"
    })
    
    graph.add_edge("search_rules", "summarize")
    graph.add_edge("verify_rules", "suggest_fix")
    graph.add_edge("summarize", "stream_output")
    graph.add_edge("suggest_fix", "stream_output")
    
    # ììœ¨ ë£¨í”„
    graph.add_edge("think", "plan")
    graph.add_edge("plan", "act")
    graph.add_edge("act", "observe")
    graph.add_conditional_edges("observe", should_continue, {
        "continue": "think",
        "complete": "complete"
    })
    graph.add_edge("complete", "stream_output")
    
    graph.add_edge("stream_output", END)
    
    return graph.compile()
```
